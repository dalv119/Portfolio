# Настройка потоковой обработки данных для агрегатора доставки еды

## Описание проекта
Создайте сервис потоковой обработки данных, с помощью которого бизнес сможет протестировать новую разработку — подписку на рестораны, благодаря которой подписчики получат эксклюзивные акции на блюда ресторана.

## Задачи проекта
Написать сервис, который будет:
1. Читать данные из Kafka с помощью Spark Structured Streaming и Python в режиме реального времени.
2. Получать список подписчиков из базы данных Postgres. 
3. Джойнить данные из Kafka с данными из БД.
4. Сохранять в памяти полученные данные, чтобы не собирать их заново после отправки в Postgres или Kafka.
5. Отправлять выходное сообщение в Kafka с информацией об акции, пользователе со списком избранного и ресторане.
6. Вставлять записи в Postgres, чтобы получить фидбэк от пользователя. 
   
## Инструменты
> Kafka, Spark Streaming, PySpark, PostgreSQL, Python
### Навыки
- построение системы потоковой обработки с использованием Apache Spark Structured Streaming;
- работа с брокером сообщений Kafka; 
- объединение потоковых и статических данных;
- дедупликация данных при потоковой обработке.

## Общий вывод
Благодаря созданному сервису рестораны смогут привлечь новую аудиторию, получить фидбэк на новые блюда и акции, продать профицит товара и увеличить продажи в непиковые часы. 
